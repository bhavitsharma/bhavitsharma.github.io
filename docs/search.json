[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "TODO(Bhavit)"
  },
  {
    "objectID": "posts/cs229-notes-p1/index.html",
    "href": "posts/cs229-notes-p1/index.html",
    "title": "cs229 notes",
    "section": "",
    "text": "Good link: https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\n\n\n\n\\[\\frac{\\partial}{\\partial X} \\log |X| = X^{-1}\\]\n\nProof: \\[\\frac{\\partial}{\\partial X} \\log |X| = \\frac{1}{|X|} \\frac{\\partial |X|}{\\partial X}\\]\nWe know that \\[\n(\\frac{\\partial |X|}{\\partial X})_{ij} = \\frac{\\partial}{\\partial X_{ij}} * det(X)\n\\]\nand \\[\ndet(X) = X_{i1} C_{i1} + X_{i2} C_{i2} + \\dots + X_{in} C_{in}\n\\] where \\(C_{ij}\\) is the cofactor of \\(X_{ij}\\). So, \\[\n\\frac{\\partial}{\\partial X_{ij}} * det(X) = C_{ij}\n\\] \\[\n\\frac{\\partial |X|}{\\partial X} = C = adj(X)^T\n\\] where \\(C\\) is the cofactor matrix of \\(X\\). \\(adj(X)\\) is the adjugate matrix of \\(X\\) and \\(X^{-1} = \\frac{adjX}{|X|}\\).\nso we get \\[\n\\frac{\\partial}{\\partial X} \\log |X| = \\frac{1}{|X|} \\frac{\\partial |X|}{\\partial X} = \\frac{1}{|X|} adj(X)^T = ({X^{-1}})^T\n\\]\nReference: kamper matrix calculus\n\n\\(\\frac{\\partial}{\\partial X} (z^TX^{-1}z) = -(X^{-1})zz^T(X^{-1})\\)\n\nProof: \\[\n\\frac{\\partial}{\\partial X} (z^TX^{-1}z)\n\\]\nLets first compute the derivative of \\(z^TX^{-1}z\\) with respect to \\(X_{ij}\\) \\[\n\\frac{\\partial}{\\partial X_{ij}} (z^TX^{-1}z)\n\\]\nLets first derive \\(\\frac{\\partial X^{-1}}{\\partial X_{ij}}\\) \\[\n\\frac{\\partial X^{-1}}{\\partial X_{ij}}\n\\] Using \\(X * X^{-1} = I\\) we get \\[\nX^{-1}\\frac{\\partial X}{\\partial X_{ij}} + \\frac{\\partial X^{-1}}{\\partial X_{ij}}X = 0\n\\] i.e. \\[\n\\frac{\\partial X^{-1}}{\\partial X_{ij}} = -X^{-1}\\frac{\\partial X}{\\partial X_{ij}}X^{-1}\n\\] where \\(\\frac{\\partial X}{\\partial X_{ij}}\\) is the matrix of partial derivatives of \\(X\\) with respect to \\(X_{ij}\\) and it’s elements are \\(0\\) except for the element at \\(i,j\\) which is \\(1\\).\nSo lets say \\(H = \\frac{\\partial\\ tr(z^TX^{-1}z)}{\\partial X}\\) \\[\nH_{ij} = \\frac{\\partial}{\\partial X_{ij}} tr(z^TX^{-1}z)\n\\] Using cyclic property of trace we get \\[\nH_{ij} = \\frac{\\partial}{\\partial X_{ij}} tr(z^TX^{-1}z) = \\frac{\\partial}{\\partial X_{ij}} tr(zz^T(X^{-1}))\n\\]\nWe know that \\[\n\\partial(Tr(A)) = Tr(\\partial(A))\n\\] because trace is linear. so \\[\nH_{ij} = tr(zz^T\\frac{\\partial}{\\partial X_{ij}}(X^{-1})) = tr(zz^T(-X^{-1}\\frac{\\partial X}{\\partial X_{ij}}X^{-1}))\n\\] Using cyclic property of trace we get\n\\[\nH_{ij} = tr(X^{-1}zz^TX^{-1}\\frac{\\partial X}{\\partial X_{ij}})\n\\]\nNow suppose that \\[\nF = X^{-1}zz^TX^{-1}\n\\] then \\[\ntr(F\\frac{\\partial X}{\\partial X_{ij}}) = F_{ji} = F_{ij}\n\\] since \\(F\\) is symmetric. Hint: You can think of the fact only the \\(jth\\) row of \\(F\\) is multiplied by the \\(jth\\) column, and only \\(ith\\) column of \\(jth\\) row of \\(F\\) is multiplied by the \\(ith\\) row of \\(jth\\) column of \\(F\\) leading to element at \\(F_{jj}\\) contributing and the rest being zero.\nHence: \\(H = -X^{-1}zz^TX^{-1}\\)"
  },
  {
    "objectID": "posts/cs229-notes-p1/index.html#some-useful-identities",
    "href": "posts/cs229-notes-p1/index.html#some-useful-identities",
    "title": "cs229 notes",
    "section": "",
    "text": "\\[\\frac{\\partial}{\\partial X} \\log |X| = X^{-1}\\]\n\nProof: \\[\\frac{\\partial}{\\partial X} \\log |X| = \\frac{1}{|X|} \\frac{\\partial |X|}{\\partial X}\\]\nWe know that \\[\n(\\frac{\\partial |X|}{\\partial X})_{ij} = \\frac{\\partial}{\\partial X_{ij}} * det(X)\n\\]\nand \\[\ndet(X) = X_{i1} C_{i1} + X_{i2} C_{i2} + \\dots + X_{in} C_{in}\n\\] where \\(C_{ij}\\) is the cofactor of \\(X_{ij}\\). So, \\[\n\\frac{\\partial}{\\partial X_{ij}} * det(X) = C_{ij}\n\\] \\[\n\\frac{\\partial |X|}{\\partial X} = C = adj(X)^T\n\\] where \\(C\\) is the cofactor matrix of \\(X\\). \\(adj(X)\\) is the adjugate matrix of \\(X\\) and \\(X^{-1} = \\frac{adjX}{|X|}\\).\nso we get \\[\n\\frac{\\partial}{\\partial X} \\log |X| = \\frac{1}{|X|} \\frac{\\partial |X|}{\\partial X} = \\frac{1}{|X|} adj(X)^T = ({X^{-1}})^T\n\\]\nReference: kamper matrix calculus\n\n\\(\\frac{\\partial}{\\partial X} (z^TX^{-1}z) = -(X^{-1})zz^T(X^{-1})\\)\n\nProof: \\[\n\\frac{\\partial}{\\partial X} (z^TX^{-1}z)\n\\]\nLets first compute the derivative of \\(z^TX^{-1}z\\) with respect to \\(X_{ij}\\) \\[\n\\frac{\\partial}{\\partial X_{ij}} (z^TX^{-1}z)\n\\]\nLets first derive \\(\\frac{\\partial X^{-1}}{\\partial X_{ij}}\\) \\[\n\\frac{\\partial X^{-1}}{\\partial X_{ij}}\n\\] Using \\(X * X^{-1} = I\\) we get \\[\nX^{-1}\\frac{\\partial X}{\\partial X_{ij}} + \\frac{\\partial X^{-1}}{\\partial X_{ij}}X = 0\n\\] i.e. \\[\n\\frac{\\partial X^{-1}}{\\partial X_{ij}} = -X^{-1}\\frac{\\partial X}{\\partial X_{ij}}X^{-1}\n\\] where \\(\\frac{\\partial X}{\\partial X_{ij}}\\) is the matrix of partial derivatives of \\(X\\) with respect to \\(X_{ij}\\) and it’s elements are \\(0\\) except for the element at \\(i,j\\) which is \\(1\\).\nSo lets say \\(H = \\frac{\\partial\\ tr(z^TX^{-1}z)}{\\partial X}\\) \\[\nH_{ij} = \\frac{\\partial}{\\partial X_{ij}} tr(z^TX^{-1}z)\n\\] Using cyclic property of trace we get \\[\nH_{ij} = \\frac{\\partial}{\\partial X_{ij}} tr(z^TX^{-1}z) = \\frac{\\partial}{\\partial X_{ij}} tr(zz^T(X^{-1}))\n\\]\nWe know that \\[\n\\partial(Tr(A)) = Tr(\\partial(A))\n\\] because trace is linear. so \\[\nH_{ij} = tr(zz^T\\frac{\\partial}{\\partial X_{ij}}(X^{-1})) = tr(zz^T(-X^{-1}\\frac{\\partial X}{\\partial X_{ij}}X^{-1}))\n\\] Using cyclic property of trace we get\n\\[\nH_{ij} = tr(X^{-1}zz^TX^{-1}\\frac{\\partial X}{\\partial X_{ij}})\n\\]\nNow suppose that \\[\nF = X^{-1}zz^TX^{-1}\n\\] then \\[\ntr(F\\frac{\\partial X}{\\partial X_{ij}}) = F_{ji} = F_{ij}\n\\] since \\(F\\) is symmetric. Hint: You can think of the fact only the \\(jth\\) row of \\(F\\) is multiplied by the \\(jth\\) column, and only \\(ith\\) column of \\(jth\\) row of \\(F\\) is multiplied by the \\(ith\\) row of \\(jth\\) column of \\(F\\) leading to element at \\(F_{jj}\\) contributing and the rest being zero.\nHence: \\(H = -X^{-1}zz^TX^{-1}\\)"
  },
  {
    "objectID": "posts/cs229-notes-p1/index.html#convex-function",
    "href": "posts/cs229-notes-p1/index.html#convex-function",
    "title": "cs229 notes",
    "section": "Convex function",
    "text": "Convex function\nA function \\(f\\) is convex if for any \\(x,y \\in \\mathbb{R}^n\\) and \\(\\alpha \\in [0,1]\\) we have \\[\nf(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha)f(y)\n\\]\nThis basically means that any line segment between two points on the graph of the function lies above the graph of the function."
  },
  {
    "objectID": "posts/cs229-notes-p1/index.html#convex-functions-properties",
    "href": "posts/cs229-notes-p1/index.html#convex-functions-properties",
    "title": "cs229 notes",
    "section": "Convex functions properties",
    "text": "Convex functions properties\nProperty 1: If \\(f\\) is convex then \\(f(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha)f(y)\\) for any \\(\\alpha \\in [0,1]\\) and \\(x,y \\in \\mathbb{R}^n\\).\nProof: Let \\(x,y \\in \\mathbb{R}\\) and \\(\\alpha \\in [0,1]\\). Since any point between \\(x\\) and \\(y\\) on the line segment \\([x,y]\\) is given by \\(\\alpha x + (1-\\alpha)y\\), we have the following from the definition of convexity:\nThe value of function as point \\(z = \\alpha x + (1-\\alpha)y\\) is \\(f(\\alpha x + (1-\\alpha)y)\\). Now the equation of line is: \\[\ny = y_1 + \\frac{y_2 - y_1}{x_2 - x_1}(x - x_1)\n\\] Plugging values for \\(z\\) and \\(x_1, x_2, y_1, y_2\\) we get:\n\\[\n\\begin{split}\ny & = f(x) + \\frac{f(y) - f(x)}{y - x}(\\alpha.x + (1 - \\alpha)y - x) \\\\\n  &= f(x) + \\frac{f(y) - f(x)}{y - x}(y - x)(1 - \\alpha) \\\\\n  &= f(x) (1 - 1 + \\alpha) + f(y)(1 - \\alpha) \\\\\n  &= f(x) (\\alpha) + f(y)(1 - \\alpha) \\\\\n\\end{split}\n\\]\nSo according to the definition of convexity we have: \\[\nf(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha)f(y)\n\\]\nProperty 2: Suppose \\(f: \\mathcal{R}^n \\rightarrow \\mathcal{R}\\). Then\n\n\\(f(y) \\geq f(x) + \\nabla f(x)^T(y-x)\\) for all \\(x,y \\in \\mathcal{R}^n\\) if and only if \\(f\\) is convex\n\\(\\nabla^2 \\succeq 0\\) if and only if \\(f\\) is convex\n\nProof:\n\n\n\n\n\n\n\n\n\n\nUsing the definition of convexity we have: \\[\n\\begin{split}\nf(\\alpha x + (1-\\alpha)y) &\\leq \\alpha f(x) + (1-\\alpha)f(y) \\\\\nf(y) - f(x) \\geq \\frac{f(x + \\alpha(y - x)) - f(x)}{\\alpha} \\\\\n\\text{if $\\alpha \\rightarrow$ 0} \\\\\nf(y) - f(x) \\geq \\nabla f(x)^T(y - x) \\\\\n\\end{split}\n\\]\n\nNow we also need to prove the other direction. So suppose \\(f(y) \\geq f(x) + \\nabla f(x)^T(y-x)\\) for all \\(x,y \\in \\mathcal{R}^n\\). We need to prove that \\(f(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha)f(y)\\). Let’s consider \\(z = \\alpha x + (1-\\alpha)y\\). Then we have:\n\\[\n\\begin{split}\nf(x) &\\geq f(z) + \\nabla f(z)^T(x - z) \\\\\nf(y) &\\geq f(z) + \\nabla f(z)^T(y - z) \\\\\n\\text{Multiply first with $\\alpha$ and other by $1 - \\alpha$ and add} \\\\\n\\alpha f(x) + (1 - \\alpha)f(y) &\\geq \\alpha f(z) + (1 - \\alpha)f(z) + \\\\ \\alpha \\nabla f(z)^T(x - z) + (1 - \\alpha)\\nabla f(z)^T(y - z) \\\\\n  &\\geq f(z) + \\nabla f(z)^T(\\alpha x + (1 - \\alpha)y - z) \\\\\n\\text{Since $\\alpha x + (1 - \\alpha)y = z$ } \\\\\n  &\\geq f(z) + \\nabla f(z)^T(0) \\\\\n  &\\geq f(z) \\\\\n\\end{split}\n\\]\n\nLet’s prove the second part. Suppose \\(\\nabla^2 \\succeq 0\\) then we have:\n\nLet us first prove it for \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\). Let \\(x, y \\in dom(f)\\) and \\(x \\leq y\\) then we have: \\[\n\\begin{split}\nf(y) - f(x) &\\geq f'(x)(y - x) \\\\\nf(x) - f(y) &\\geq f'(y)(x - y) \\\\\n\\implies \\frac{f'(x) - f'(y)}{x - y} &\\geq 0 \\\\\n\\text{if $x \\rightarrow y$ then} \\\\\n\\frac{f'(x) - f'(y)}{x - y} \\rightarrowtail f''(x) \\geq 0 \\\\\n\\end{split}\n\\]\nWe can prove the other direction using the mean value version of the Taylor’s theorem. Suppose \\(f''(x) \\geq 0\\) then there exists a point \\(z \\in (x, y)\\) such that:\n\\[\n\\begin{split}\nf(y) = f(x) + f'(x)(y - x) + f''(z)\\frac{(y - x)^2}{2} \\\\\n\\text{Since $f''(z) \\geq 0$} \\\\\nf(y) \\geq f(x) + f'(x)(y - x) \\\\\n\\end{split}\n\\]\nNow we need to prove the same for \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\). Remember that a convex function is convex along all lines. i.e. if \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex then \\(g(\\alpha) = f(x + \\alpha(v))\\) is convex for all \\(x \\in \\mathbb{R}^n\\) and \\(v \\in \\mathbb{R}^n\\).\n\\[\n\\begin{split}\ng''(\\alpha) = v^T\\nabla^2 f(x + \\alpha v) v \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bhavit’s blog",
    "section": "",
    "text": "Risk and Learning\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nBhavit Sharma\n\n\n\n\n\n\n\n\n\n\n\n\ncs229 notes\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nBhavit Sharma\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/risk-and-bias/index.html",
    "href": "posts/risk-and-bias/index.html",
    "title": "Risk and Learning",
    "section": "",
    "text": "Note\n\n\n\nWe’re going to show that empirical risk is an unbiased estimator of the expected risk.\nIn statistics, if you have a quantity \\(\\theta\\) that you want to estimate, but you can only estimate it via some fancy algorithms and random observations. Say you come up with the estimator \\(\\hat{\\theta}\\), then \\(\\hat{\\theta}\\) is an unbiased estimator of \\(\\theta\\) if:\n\\[\n\\mathbb{E}[\\hat{\\theta}] = \\theta\n\\]\nIn general, bias of this estimate \\(\\hat{\\theta}\\) is defined as: \\[\n\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta\n\\]"
  },
  {
    "objectID": "posts/risk-and-bias/index.html#finite-hypothesis-class",
    "href": "posts/risk-and-bias/index.html#finite-hypothesis-class",
    "title": "Risk and Learning",
    "section": "Finite Hypothesis Class",
    "text": "Finite Hypothesis Class\nWe’re going to look at one proof which says we’re guaranteed to minimize ERM if we have a finite hypothesis class and our true generating function is in the hypothesis class. However, we’re going to make some strong assumptions for this proof, including the loss function and the problem setting.\n\n\n\n\n\n\nImportant\n\n\n\nThe assumptions are really important!! We’re going to relax them in the subsequent sections.\n\n\n\nWe’re looking at a binary classification problem, where \\(Y = \\{-1, 1\\}\\).\nThe loss function is defined as the fraction of misclassified examples. Let say our distribution is \\(\\mathcal{D}\\) and \\(S\\) is a sampled training set from \\(\\mathcal{D}\\), and we have a predictor \\(h_S: X \\to Y\\) which is the output of the ERM algorithm. Then the loss function is defined as: \\[\n\\mathcal{L_s}(h) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(h(x_i) \\neq y_i)\n\\] where \\(\\mathbb{I}\\) is the indicator function.\nWe’re assuming that the true generating function \\(f\\) is in the hypothesis class \\(\\mathcal{H}\\).\n\\(|\\mathcal{H}|\\) is finite i.e. the hypothesis class is finite."
  },
  {
    "objectID": "posts/risk-and-bias/index.html#proof-sketch",
    "href": "posts/risk-and-bias/index.html#proof-sketch",
    "title": "Risk and Learning",
    "section": "Proof Sketch",
    "text": "Proof Sketch\nWe know we’ll pick the hypothesis \\(h_S\\) that minimizes the empirical risk for the training set \\(S\\). So \\[\nh_S \\in \\arg \\min_{h \\in \\mathcal{H}} \\mathcal{L}_S(h)\n\\]\nWe can also pick a bad hypothesis \\(h\\) from the hypothesis class \\(\\mathcal{H}\\) such that \\(\\mathcal{L}_S(h) = 0\\) - for example - consider a hypothesis that predicts labels given in the training set as is. i.e.\n\n\\[\nh_S = \\begin{cases}\ny_i & \\text{if there exists } i \\in [n] \\text{ such that } x_i = x \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nThis is saying if I get the same training example, I’ll predict the same label as in the training set, otherwise I’ll predict 0. This obviously is a bad hypothesis, but it has zero empirical risk.\nWe want to show in the proof that the probability of picking such a bad hypothesis is low (those who have 0 empirical risk) in our setting.\n\nSince, \\(f \\in \\mathcal{H}\\), we know that true risk of \\(f\\) is 0, which means when searching for the best hypothesis we can ignore any hypothesis that has a non-zero empirical risk.\nNow that we’ve picked the best hypothesis \\(h_S\\) who has an empirical risk of 0, there can be bad hypothesis \\(h\\) such that \\(\\mathcal{L}_S(h) = 0\\) (an example is shown above) and good hypothesis \\(h\\) such that \\(\\mathcal{L}_S(h) = 0\\).\nWe’ll also denote a quantity \\(\\mathcal{L}_{\\mathcal{D}, f}(h)\\) which is the true risk of the hypothesis \\(h\\) with respect to the true distribution \\(\\mathcal{D}\\) and the true generating function \\(f\\).\nClearly for bad hypothesis \\(h\\), \\(\\mathcal{L}_{\\mathcal{D}, f}(h) &gt; 0\\). Since our loss function is the fraction of misclassified examples, we’ll say that our hypothesis \\(h\\) is bad if \\(\\mathcal{L}_{\\mathcal{D}, f}(h) &gt; \\epsilon\\) for some \\(\\epsilon &gt; 0\\). \\(\\epsilon\\) is the margin of error we’re willing to tolerate.\nThis means that good hypothesis \\(h\\) will have \\(\\mathcal{L}_{\\mathcal{D}, f}(h) \\leq \\epsilon\\).\nTherefore, we’re interested in upper bounding the probability of picking a bad hypothesis \\(h\\).\nPoints in the training sample \\(S\\) are drawn i.i.d from the true distribution \\(\\mathcal{D}\\)."
  },
  {
    "objectID": "posts/risk-and-bias/index.html#proof",
    "href": "posts/risk-and-bias/index.html#proof",
    "title": "Risk and Learning",
    "section": "Proof",
    "text": "Proof\nGiven our proof outline, lets formalize the proof.\n\n\n\n\n\n\nImportant\n\n\n\nRemember that we’re picking the best hypothesis \\(h_S\\) that minimizes the empirical risk for the training set \\(S\\). So it’s possible we might pick a bad hypothesis \\(h\\) such that \\(\\mathcal{L}_S(h) = 0\\). It also means we might a good hypothesis \\(h\\) such that \\(\\mathcal{L}_S(h) = 0\\).\n\n\nWe want to upper bound this quantity: \\[\nPr[S : \\mathcal{L}_{\\mathcal{D}, f}(h_S) &gt; \\epsilon]\n\\]\nThis basically says that the probability of the samples \\(S\\) such that the true risk of the hypothesis \\(h_S\\) is greater than \\(\\epsilon\\) is upper bounded by some quantity.\nNow lets say define the \\(H_B\\) as the set of bad hypothesis (all possible) \\[\nH_B = \\{h \\in \\mathcal{H} : \\mathcal{L}_{\\mathcal{D}, f}(h) &gt; \\epsilon\\}\n\\]\nNow lets consider the set of all possible training sets \\(S\\) which contains atleast one bad hypothesis \\(h \\in H_B\\):\n\\[\nM = \\{S : \\exists h \\in H_B \\text{ such that } \\mathcal{L}_S(h) = 0\\}\n\\]\nNow we’re going to use a trick in probability theory to upper bound the probability of \\(M\\). We can write \\(M\\) as a union over all bad hypothesis \\(h \\in H_B\\):\n\\[\nM = \\bigcup_{h \\in H_B} \\{S : \\mathcal{L}_S(h) = 0\\}\n\\]\nNow we can use the union bound to upper bound the probability of \\(M\\):\n\\[\nPr[S: \\mathcal{L}_{\\mathcal{D}, f}(h_S) &gt; \\epsilon] \\leq Pr[M] \\leq \\sum_{h \\in H_B} Pr[\\{S : \\mathcal{L}_S(h) = 0\\}]\n\\]\nThis is because \\(M\\) is superset of all the events \\(\\{S : \\mathcal{L}_S(h) = 0\\}\\). and also \\(Pr(A \\cup B) \\leq Pr(A) + Pr(B)\\).\nNow, for a single hypothesis \\(h \\in H_B\\), we can upper bound the probability of picking a training set \\(S\\) such that \\(\\mathcal{L}_S(h) = 0\\) which means for training points \\(x_i\\) such that \\(h(x_i) = y_i\\) for all \\(1 \\leq i \\leq n\\).\nSince \\(h\\) is a bad hypothesis, we know that \\(\\mathcal{L}_{\\mathcal{D}, f}(h) &gt; \\epsilon\\), this means the \\(Pr[h[x_i] = y_i] \\leq 1 - \\epsilon\\) for all \\(1 \\leq i \\leq n\\).\nNow, combining all the above, we get: \\[\nPr[S: \\mathcal{L}_S(h) = 0] = \\prod_{i=1}^n Pr[h(x_i) = y_i] \\leq (1 - \\epsilon)^n\n\\]\nSo,\n\\[\nPr[S: \\mathcal{L}_{\\mathcal{D}, f}(h_S) &gt; \\epsilon] \\leq \\sum_{h \\in H_B} (1 - \\epsilon)^n \\\\\n\\leq |H_B| (1 - \\epsilon)^n\n\\]\nSince \\(|H_B|\\) is finite, we can choose \\(n\\) large enough such that \\((1 - \\epsilon)^n\\) is small enough and tends to 0.\nReferences: https://www.wikiwand.com/en/Empirical_risk_minimization"
  }
]