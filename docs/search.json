[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "TODO(Bhavit)"
  },
  {
    "objectID": "posts/cs229-notes-p1/index.html",
    "href": "posts/cs229-notes-p1/index.html",
    "title": "cs229 notes",
    "section": "",
    "text": "Good link: https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\n\n\n\n\\[\\frac{\\partial}{\\partial X} \\log |X| = X^{-1}\\]\n\nProof: \\[\\frac{\\partial}{\\partial X} \\log |X| = \\frac{1}{|X|} \\frac{\\partial |X|}{\\partial X}\\]\nWe know that \\[\n(\\frac{\\partial |X|}{\\partial X})_{ij} = \\frac{\\partial}{\\partial X_{ij}} * det(X)\n\\]\nand \\[\ndet(X) = X_{i1} C_{i1} + X_{i2} C_{i2} + \\dots + X_{in} C_{in}\n\\] where \\(C_{ij}\\) is the cofactor of \\(X_{ij}\\). So, \\[\n\\frac{\\partial}{\\partial X_{ij}} * det(X) = C_{ij}\n\\] \\[\n\\frac{\\partial |X|}{\\partial X} = C = adj(X)^T\n\\] where \\(C\\) is the cofactor matrix of \\(X\\). \\(adj(X)\\) is the adjugate matrix of \\(X\\) and \\(X^{-1} = \\frac{adjX}{|X|}\\).\nso we get \\[\n\\frac{\\partial}{\\partial X} \\log |X| = \\frac{1}{|X|} \\frac{\\partial |X|}{\\partial X} = \\frac{1}{|X|} adj(X)^T = ({X^{-1}})^T\n\\]\nReference: kamper matrix calculus\n\n\\(\\frac{\\partial}{\\partial X} (z^TX^{-1}z) = -(X^{-1})zz^T(X^{-1})\\)\n\nProof: \\[\n\\frac{\\partial}{\\partial X} (z^TX^{-1}z)\n\\]\nLets first compute the derivative of \\(z^TX^{-1}z\\) with respect to \\(X_{ij}\\) \\[\n\\frac{\\partial}{\\partial X_{ij}} (z^TX^{-1}z)\n\\]\nLets first derive \\(\\frac{\\partial X^{-1}}{\\partial X_{ij}}\\) \\[\n\\frac{\\partial X^{-1}}{\\partial X_{ij}}\n\\] Using \\(X * X^{-1} = I\\) we get \\[\nX^{-1}\\frac{\\partial X}{\\partial X_{ij}} + \\frac{\\partial X^{-1}}{\\partial X_{ij}}X = 0\n\\] i.e. \\[\n\\frac{\\partial X^{-1}}{\\partial X_{ij}} = -X^{-1}\\frac{\\partial X}{\\partial X_{ij}}X^{-1}\n\\] where \\(\\frac{\\partial X}{\\partial X_{ij}}\\) is the matrix of partial derivatives of \\(X\\) with respect to \\(X_{ij}\\) and it’s elements are \\(0\\) except for the element at \\(i,j\\) which is \\(1\\).\nSo lets say \\(H = \\frac{\\partial\\ tr(z^TX^{-1}z)}{\\partial X}\\) \\[\nH_{ij} = \\frac{\\partial}{\\partial X_{ij}} tr(z^TX^{-1}z)\n\\] Using cyclic property of trace we get \\[\nH_{ij} = \\frac{\\partial}{\\partial X_{ij}} tr(z^TX^{-1}z) = \\frac{\\partial}{\\partial X_{ij}} tr(zz^T(X^{-1}))\n\\]\nWe know that \\[\n\\partial(Tr(A)) = Tr(\\partial(A))\n\\] because trace is linear. so \\[\nH_{ij} = tr(zz^T\\frac{\\partial}{\\partial X_{ij}}(X^{-1})) = tr(zz^T(-X^{-1}\\frac{\\partial X}{\\partial X_{ij}}X^{-1}))\n\\] Using cyclic property of trace we get\n\\[\nH_{ij} = tr(X^{-1}zz^TX^{-1}\\frac{\\partial X}{\\partial X_{ij}})\n\\]\nNow suppose that \\[\nF = X^{-1}zz^TX^{-1}\n\\] then \\[\ntr(F\\frac{\\partial X}{\\partial X_{ij}}) = F_{ji} = F_{ij}\n\\] since \\(F\\) is symmetric. Hint: You can think of the fact only the \\(jth\\) row of \\(F\\) is multiplied by the \\(jth\\) column, and only \\(ith\\) column of \\(jth\\) row of \\(F\\) is multiplied by the \\(ith\\) row of \\(jth\\) column of \\(F\\) leading to element at \\(F_{jj}\\) contributing and the rest being zero.\nHence: \\(H = -X^{-1}zz^TX^{-1}\\)"
  },
  {
    "objectID": "posts/cs229-notes-p1/index.html#some-useful-identities",
    "href": "posts/cs229-notes-p1/index.html#some-useful-identities",
    "title": "cs229 notes",
    "section": "",
    "text": "\\[\\frac{\\partial}{\\partial X} \\log |X| = X^{-1}\\]\n\nProof: \\[\\frac{\\partial}{\\partial X} \\log |X| = \\frac{1}{|X|} \\frac{\\partial |X|}{\\partial X}\\]\nWe know that \\[\n(\\frac{\\partial |X|}{\\partial X})_{ij} = \\frac{\\partial}{\\partial X_{ij}} * det(X)\n\\]\nand \\[\ndet(X) = X_{i1} C_{i1} + X_{i2} C_{i2} + \\dots + X_{in} C_{in}\n\\] where \\(C_{ij}\\) is the cofactor of \\(X_{ij}\\). So, \\[\n\\frac{\\partial}{\\partial X_{ij}} * det(X) = C_{ij}\n\\] \\[\n\\frac{\\partial |X|}{\\partial X} = C = adj(X)^T\n\\] where \\(C\\) is the cofactor matrix of \\(X\\). \\(adj(X)\\) is the adjugate matrix of \\(X\\) and \\(X^{-1} = \\frac{adjX}{|X|}\\).\nso we get \\[\n\\frac{\\partial}{\\partial X} \\log |X| = \\frac{1}{|X|} \\frac{\\partial |X|}{\\partial X} = \\frac{1}{|X|} adj(X)^T = ({X^{-1}})^T\n\\]\nReference: kamper matrix calculus\n\n\\(\\frac{\\partial}{\\partial X} (z^TX^{-1}z) = -(X^{-1})zz^T(X^{-1})\\)\n\nProof: \\[\n\\frac{\\partial}{\\partial X} (z^TX^{-1}z)\n\\]\nLets first compute the derivative of \\(z^TX^{-1}z\\) with respect to \\(X_{ij}\\) \\[\n\\frac{\\partial}{\\partial X_{ij}} (z^TX^{-1}z)\n\\]\nLets first derive \\(\\frac{\\partial X^{-1}}{\\partial X_{ij}}\\) \\[\n\\frac{\\partial X^{-1}}{\\partial X_{ij}}\n\\] Using \\(X * X^{-1} = I\\) we get \\[\nX^{-1}\\frac{\\partial X}{\\partial X_{ij}} + \\frac{\\partial X^{-1}}{\\partial X_{ij}}X = 0\n\\] i.e. \\[\n\\frac{\\partial X^{-1}}{\\partial X_{ij}} = -X^{-1}\\frac{\\partial X}{\\partial X_{ij}}X^{-1}\n\\] where \\(\\frac{\\partial X}{\\partial X_{ij}}\\) is the matrix of partial derivatives of \\(X\\) with respect to \\(X_{ij}\\) and it’s elements are \\(0\\) except for the element at \\(i,j\\) which is \\(1\\).\nSo lets say \\(H = \\frac{\\partial\\ tr(z^TX^{-1}z)}{\\partial X}\\) \\[\nH_{ij} = \\frac{\\partial}{\\partial X_{ij}} tr(z^TX^{-1}z)\n\\] Using cyclic property of trace we get \\[\nH_{ij} = \\frac{\\partial}{\\partial X_{ij}} tr(z^TX^{-1}z) = \\frac{\\partial}{\\partial X_{ij}} tr(zz^T(X^{-1}))\n\\]\nWe know that \\[\n\\partial(Tr(A)) = Tr(\\partial(A))\n\\] because trace is linear. so \\[\nH_{ij} = tr(zz^T\\frac{\\partial}{\\partial X_{ij}}(X^{-1})) = tr(zz^T(-X^{-1}\\frac{\\partial X}{\\partial X_{ij}}X^{-1}))\n\\] Using cyclic property of trace we get\n\\[\nH_{ij} = tr(X^{-1}zz^TX^{-1}\\frac{\\partial X}{\\partial X_{ij}})\n\\]\nNow suppose that \\[\nF = X^{-1}zz^TX^{-1}\n\\] then \\[\ntr(F\\frac{\\partial X}{\\partial X_{ij}}) = F_{ji} = F_{ij}\n\\] since \\(F\\) is symmetric. Hint: You can think of the fact only the \\(jth\\) row of \\(F\\) is multiplied by the \\(jth\\) column, and only \\(ith\\) column of \\(jth\\) row of \\(F\\) is multiplied by the \\(ith\\) row of \\(jth\\) column of \\(F\\) leading to element at \\(F_{jj}\\) contributing and the rest being zero.\nHence: \\(H = -X^{-1}zz^TX^{-1}\\)"
  },
  {
    "objectID": "posts/cs229-notes-p1/index.html#convex-function",
    "href": "posts/cs229-notes-p1/index.html#convex-function",
    "title": "cs229 notes",
    "section": "Convex function",
    "text": "Convex function\nA function \\(f\\) is convex if for any \\(x,y \\in \\mathbb{R}^n\\) and \\(\\alpha \\in [0,1]\\) we have \\[\nf(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha)f(y)\n\\]\nThis basically means that any line segment between two points on the graph of the function lies above the graph of the function."
  },
  {
    "objectID": "posts/cs229-notes-p1/index.html#convex-functions-properties",
    "href": "posts/cs229-notes-p1/index.html#convex-functions-properties",
    "title": "cs229 notes",
    "section": "Convex functions properties",
    "text": "Convex functions properties\nProperty 1: If \\(f\\) is convex then \\(f(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha)f(y)\\) for any \\(\\alpha \\in [0,1]\\) and \\(x,y \\in \\mathbb{R}^n\\).\nProof: Let \\(x,y \\in \\mathbb{R}\\) and \\(\\alpha \\in [0,1]\\). Since any point between \\(x\\) and \\(y\\) on the line segment \\([x,y]\\) is given by \\(\\alpha x + (1-\\alpha)y\\), we have the following from the definition of convexity:\nThe value of function as point \\(z = \\alpha x + (1-\\alpha)y\\) is \\(f(\\alpha x + (1-\\alpha)y)\\). Now the equation of line is: \\[\ny = y_1 + \\frac{y_2 - y_1}{x_2 - x_1}(x - x_1)\n\\] Plugging values for \\(z\\) and \\(x_1, x_2, y_1, y_2\\) we get:\n\\[\n\\begin{split}\ny & = f(x) + \\frac{f(y) - f(x)}{y - x}(\\alpha.x + (1 - \\alpha)y - x) \\\\\n  &= f(x) + \\frac{f(y) - f(x)}{y - x}(y - x)(1 - \\alpha) \\\\\n  &= f(x) (1 - 1 + \\alpha) + f(y)(1 - \\alpha) \\\\\n  &= f(x) (\\alpha) + f(y)(1 - \\alpha) \\\\\n\\end{split}\n\\]\nSo according to the definition of convexity we have: \\[\nf(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha)f(y)\n\\]\nProperty 2: Suppose \\(f: \\mathcal{R}^n \\rightarrow \\mathcal{R}\\). Then\n\n\\(f(y) \\geq f(x) + \\nabla f(x)^T(y-x)\\) for all \\(x,y \\in \\mathcal{R}^n\\) if and only if \\(f\\) is convex\n\\(\\nabla^2 \\succeq 0\\) if and only if \\(f\\) is convex\n\nProof:\n\n\n\n\n\n\nUsing the definition of convexity we have: \\[\n\\begin{split}\nf(\\alpha x + (1-\\alpha)y) &\\leq \\alpha f(x) + (1-\\alpha)f(y) \\\\\nf(y) - f(x) \\geq \\frac{f(x + \\alpha(y - x)) - f(x)}{\\alpha} \\\\\n\\text{if $\\alpha \\rightarrow$ 0} \\\\\nf(y) - f(x) \\geq \\nabla f(x)^T(y - x) \\\\\n\\end{split}\n\\]\n\nNow we also need to prove the other direction. So suppose \\(f(y) \\geq f(x) + \\nabla f(x)^T(y-x)\\) for all \\(x,y \\in \\mathcal{R}^n\\). We need to prove that \\(f(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha)f(y)\\). Let’s consider \\(z = \\alpha x + (1-\\alpha)y\\). Then we have:\n\\[\n\\begin{split}\nf(x) &\\geq f(z) + \\nabla f(z)^T(x - z) \\\\\nf(y) &\\geq f(z) + \\nabla f(z)^T(y - z) \\\\\n\\text{Multiply first with $\\alpha$ and other by $1 - \\alpha$ and add} \\\\\n\\alpha f(x) + (1 - \\alpha)f(y) &\\geq \\alpha f(z) + (1 - \\alpha)f(z) + \\\\ \\alpha \\nabla f(z)^T(x - z) + (1 - \\alpha)\\nabla f(z)^T(y - z) \\\\\n  &\\geq f(z) + \\nabla f(z)^T(\\alpha x + (1 - \\alpha)y - z) \\\\\n\\text{Since $\\alpha x + (1 - \\alpha)y = z$ } \\\\\n  &\\geq f(z) + \\nabla f(z)^T(0) \\\\\n  &\\geq f(z) \\\\\n\\end{split}\n\\]\n\nLet’s prove the second part. Suppose \\(\\nabla^2 \\succeq 0\\) then we have:\n\nLet us first prove it for \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\). Let \\(x, y \\in dom(f)\\) and \\(x \\leq y\\) then we have: \\[\n\\begin{split}\nf(y) - f(x) &\\geq f'(x)(y - x) \\\\\nf(x) - f(y) &\\geq f'(y)(x - y) \\\\\n\\implies \\frac{f'(x) - f'(y)}{x - y} &\\geq 0 \\\\\n\\text{if $x \\rightarrow y$ then} \\\\\n\\frac{f'(x) - f'(y)}{x - y} \\rightarrowtail f''(x) \\geq 0 \\\\\n\\end{split}\n\\]\nWe can prove the other direction using the mean value version of the Taylor’s theorem. Suppose \\(f''(x) \\geq 0\\) then there exists a point \\(z \\in (x, y)\\) such that:\n\\[\n\\begin{split}\nf(y) = f(x) + f'(x)(y - x) + f''(z)\\frac{(y - x)^2}{2} \\\\\n\\text{Since $f''(z) \\geq 0$} \\\\\nf(y) \\geq f(x) + f'(x)(y - x) \\\\\n\\end{split}\n\\]\nNow we need to prove the same for \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\). Remember that a convex function is convex along all lines. i.e. if \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex then \\(g(\\alpha) = f(x + \\alpha(v))\\) is convex for all \\(x \\in \\mathbb{R}^n\\) and \\(v \\in \\mathbb{R}^n\\).\n\\[\n\\begin{split}\ng''(\\alpha) = v^T\\nabla^2 f(x + \\alpha v) v \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bhavit’s blog",
    "section": "",
    "text": "cs229 notes\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nBhavit Sharma\n\n\n\n\n\n\n  \n\n\n\n\nTransformers from scratch\n\n\n\n\n\n\n\nML\n\n\nLLMs\n\n\nTransformers\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nBhavit Sharma\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/transformers-from-scratch/index.html",
    "href": "posts/transformers-from-scratch/index.html",
    "title": "Transformers from scratch",
    "section": "",
    "text": "Why?\nRecently I started learning about transformers. I found the “Attention is all you need” paper extremely hard to read given I had no background related to ML, and the paper contained a lot of concepts which made it difficult. A lot of the blogs explaining transformers straight up just jumped to explaining the architecture instead of setting up the right motivation behind various decisions, including attention.\nThis post is an attempt to bridge the gap between noobs and “Attention is all you need” paper. We’ll go through a journey spanning over a decade to understand the various decisions behind transformers. We’ll also implement toy versions of this techniques for toy datasets like machine-translation.\n\n\nBackground\nTransformers were introduced in 2017’s paper “Attention is all you need”. However, a lot of techniques behind the paper were introduced in other papers.\n\nSequence to Sequence Learning with Neural Networks Ilya et al\nLearning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\nNeural Machine Translation by Jointly Learning to Align and Translate\n\n\n\n\nRNNs\nRNNs were one of the first architectures used to introduce long-range dependencies among sequences.\n\\(x^2=2\\)"
  }
]